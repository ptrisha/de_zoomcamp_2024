{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b04d9c5-0981-4095-b211-839de98e85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b26ce5a3-3bc9-47f0-bcdd-4acc66cd7a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the version\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333c8d67-dd26-44bf-a898-b8807e00f44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/05 19:39:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create a local spark session\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c107b8-e4b2-4afe-bae2-05b72d89acf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040765b5-b3b5-468d-a9ab-093c95b9a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-05 19:39:09--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240305%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240305T193910Z&X-Amz-Expires=300&X-Amz-Signature=d68cd19efdffea35187bf73c382d7c75e61f56abdec74f08b52e436abc7af3bc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-05 19:39:10--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240305%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240305T193910Z&X-Amz-Expires=300&X-Amz-Signature=d68cd19efdffea35187bf73c382d7c75e61f56abdec74f08b52e436abc7af3bc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19375751 (18M) [application/octet-stream]\n",
      "Saving to: ‘fhv_tripdata_2019-10.csv.gz’\n",
      "\n",
      "fhv_tripdata_2019-1 100%[===================>]  18.48M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-03-05 19:39:10 (209 MB/s) - ‘fhv_tripdata_2019-10.csv.gz’ saved [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the October 2019 FHV data\n",
    "\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed13416-41ea-4aac-9a57-7dfbcb93eb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 135936\n",
      "drwxrwxr-x  3 patlee patlee      4096 Mar  5 19:39 .\n",
      "drwxr-xr-x 17 patlee patlee      4096 Mar  5 17:57 ..\n",
      "drwxrwxr-x  2 patlee patlee      4096 Mar  5 18:00 .ipynb_checkpoints\n",
      "-rw-rw-r--  1 patlee patlee 119796110 Dec  2  2022 fhv_tripdata_2019-10.csv\n",
      "-rw-rw-r--  1 patlee patlee  19375751 Dec  2  2022 fhv_tripdata_2019-10.csv.gz\n",
      "-rw-rw-r--  1 patlee patlee      5533 Mar  5 19:35 spark_hw5.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8799a4-e087-4366-9368-8c15e76ff89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items\n",
    "pd.__version__  # iteritems has been deprecated in pandas, but this version of Spark relies on iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53cff934-9f46-45ae-a9b5-89282c4ed249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1               B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2               B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3               B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4               B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         264.0      NaN                 B00009  \n",
       "1         264.0         264.0      NaN                 B00013  \n",
       "2         264.0         264.0      NaN                 B00014  \n",
       "3         264.0         264.0      NaN                 B00014  \n",
       "4         264.0         264.0      NaN                 B00014  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to use pandas to infer the data types, by reading the first 1001 rows\n",
    "\n",
    "df_pandas = pd.read_csv(\"fhv_tripdata_2019-10.csv.gz\", compression=\"gzip\", header=0, nrows=1000)\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1285c323-9bcd-4d61-a1f8-34b77d9704e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7eab6b1-5f4c-4656-953c-9a114bc0614e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01800dd7-6f76-4171-9139-a1e95d071502",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df_pandas)\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    436\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    628\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 631\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m    632\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:517\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    515\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[1;32m    516\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 517\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    518\u001b[0m     _merge_type,\n\u001b[1;32m    519\u001b[0m     (_infer_schema(row, names, infer_dict_as_struct, prefer_timestamp_ntz) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data),\n\u001b[1;32m    520\u001b[0m )\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py:1383\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1382\u001b[0m     nfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((f\u001b[38;5;241m.\u001b[39mname, f\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cast(StructType, b)\u001b[38;5;241m.\u001b[39mfields)\n\u001b[0;32m-> 1383\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1384\u001b[0m         StructField(\n\u001b[1;32m   1385\u001b[0m             f\u001b[38;5;241m.\u001b[39mname, _merge_type(f\u001b[38;5;241m.\u001b[39mdataType, nfs\u001b[38;5;241m.\u001b[39mget(f\u001b[38;5;241m.\u001b[39mname, NullType()), name\u001b[38;5;241m=\u001b[39mnew_name(f\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[1;32m   1387\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mfields\n\u001b[1;32m   1388\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields])\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py:1385\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n\u001b[1;32m   1382\u001b[0m     nfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((f\u001b[38;5;241m.\u001b[39mname, f\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cast(StructType, b)\u001b[38;5;241m.\u001b[39mfields)\n\u001b[1;32m   1383\u001b[0m     fields \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1384\u001b[0m         StructField(\n\u001b[0;32m-> 1385\u001b[0m             f\u001b[38;5;241m.\u001b[39mname, _merge_type(f\u001b[38;5;241m.\u001b[39mdataType, nfs\u001b[38;5;241m.\u001b[39mget(f\u001b[38;5;241m.\u001b[39mname, NullType()), name\u001b[38;5;241m=\u001b[39mnew_name(f\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1386\u001b[0m         )\n\u001b[1;32m   1387\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mfields\n\u001b[1;32m   1388\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields])\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nfs:\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py:1378\u001b[0m, in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m b\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(a) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(b):\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;66;03m# TODO: type cast (such as int -> long)\u001b[39;00m\n\u001b[0;32m-> 1378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not merge type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(a), \u001b[38;5;28mtype\u001b[39m(b))))\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# same type\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, StructType):\n",
      "\u001b[0;31mTypeError\u001b[0m: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d819b205-fdb8-49a2-818e-55275aa85dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above TypeError in the column Affiliated_base_number arose because the NaN's in the column\n",
    "# are interpreted as DoubleType, while the values are StringType.  In view of this, I will write\n",
    "# the schema directly instead of using schema inference via Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "469a1184-ea24-4852-ae78-18af725ec3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f8ce8ea-eebe-4f6e-ab1b-d9e9853aeaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PUlocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOlocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b23cb280-ad38-4d2c-8f7f-7096fb809699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the fhv data using the above schema\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d33f2072-86bf-4f79-ae6d-b0d505e165e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition to 6 partitions, and save to parquet\n",
    "\n",
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a568a62b-affc-4ce9-a108-aa4233057765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93c24d6f-359e-4fc5-ae4e-c4ec4f236c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 37964\n",
      "drwxr-xr-x 2 patlee patlee    4096 Mar  5 20:23 .\n",
      "drwxr-xr-x 3 patlee patlee    4096 Mar  5 20:23 ..\n",
      "-rw-r--r-- 1 patlee patlee       8 Mar  5 20:23 ._SUCCESS.crc\n",
      "-rw-r--r-- 1 patlee patlee   50204 Mar  5 20:23 .part-00000-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 patlee patlee   50144 Mar  5 20:23 .part-00001-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 patlee patlee   50152 Mar  5 20:23 .part-00002-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 patlee patlee   50144 Mar  5 20:23 .part-00003-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 patlee patlee   50176 Mar  5 20:23 .part-00004-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 patlee patlee   50312 Mar  5 20:23 .part-00005-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 patlee patlee       0 Mar  5 20:23 _SUCCESS\n",
      "-rw-r--r-- 1 patlee patlee 6424988 Mar  5 20:23 part-00000-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet\n",
      "-rw-r--r-- 1 patlee patlee 6417333 Mar  5 20:23 part-00001-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet\n",
      "-rw-r--r-- 1 patlee patlee 6418178 Mar  5 20:23 part-00002-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet\n",
      "-rw-r--r-- 1 patlee patlee 6417268 Mar  5 20:23 part-00003-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet\n",
      "-rw-r--r-- 1 patlee patlee 6421083 Mar  5 20:23 part-00004-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet\n",
      "-rw-r--r-- 1 patlee patlee 6438857 Mar  5 20:23 part-00005-9cea4082-890d-4e38-953c-9da8ea8d0a5f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -al fhv/2019/10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f18b955f-3b39-4c25-9652-7888ba021d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many taxi trips were there on 15 October?\n",
    "\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b91e3e1-1ac1-4e94-bb20-335868034721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2019-10-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "460a832b-8dfb-4993-828c-baa78be0d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|      duration_hr|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------------+\n",
      "|              B02832|2019-10-11 18:00:00|2091-10-11 18:30:00|         264|         264|   null|                B02832|         631152.5|\n",
      "|              B02832|2019-10-28 09:00:00|2091-10-28 09:30:00|         264|         264|   null|                B02832|         631152.5|\n",
      "|              B02416|2019-10-31 23:46:33|2029-11-01 00:13:00|        null|        null|   null|                B02416|87672.44083333333|\n",
      "|     B00746         |2019-10-01 21:43:42|2027-10-01 21:45:23|         159|         264|   null|       B00746         |70128.02805555555|\n",
      "|              B02921|2019-10-17 14:00:00|2020-10-18 00:00:00|        null|        null|   null|                B03037|           8794.0|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Length of longest trip in hours\n",
    "\n",
    "df \\\n",
    "    .withColumn('duration_hr', (df.dropOff_datetime.cast('long') - df.pickup_datetime.cast('long'))/3600 ) \\\n",
    "    .orderBy('duration_hr', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "234e9e5a-5a47-48e3-80b6-818473e5ce82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-05 20:31:06--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240305%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240305T203106Z&X-Amz-Expires=300&X-Amz-Signature=bed6c57a1f66047e562c831b322be967cdafe419e2b16f358a686504f5d51d3e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-05 20:31:06--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240305%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240305T203106Z&X-Amz-Expires=300&X-Amz-Signature=bed6c57a1f66047e562c831b322be967cdafe419e2b16f358a686504f5d51d3e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-03-05 20:31:07 (70.0 MB/s) - ‘taxi_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the zones data\n",
    "\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4daa45ed-16a4-492a-835e-1a76a49585a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.csv('taxi_zone_lookup.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8acbc269-f892-417d-866b-4f19286ab279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(LocationID='1', Borough='EWR', Zone='Newark Airport', service_zone='EWR'),\n",
       " Row(LocationID='2', Borough='Queens', Zone='Jamaica Bay', service_zone='Boro Zone'),\n",
       " Row(LocationID='3', Borough='Bronx', Zone='Allerton/Pelham Gardens', service_zone='Boro Zone'),\n",
       " Row(LocationID='4', Borough='Manhattan', Zone='Alphabet City', service_zone='Yellow Zone'),\n",
       " Row(LocationID='5', Borough='Staten Island', Zone='Arden Heights', service_zone='Boro Zone')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1364ea3e-39da-48be-824d-afe74f721cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LocationID', 'Borough', 'Zone', 'service_zone']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "477170d6-8fd8-4289-b3dd-e784976bb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the zone lookup data into a temp view\n",
    "\n",
    "df_zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83a8a4cc-9026-485c-bb1e-1011223d2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fhv data into a temp view\n",
    "\n",
    "df.registerTempTable('fhv_2019_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fee3e9c8-ffe0-4bfd-821c-057eb3f9181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                Zone|count(1)|\n",
      "+--------------------+--------+\n",
      "|         Jamaica Bay|       1|\n",
      "|Governor's Island...|       2|\n",
      "| Green-Wood Cemetery|       5|\n",
      "|       Broad Channel|       8|\n",
      "|     Highbridge Park|      14|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# use zone data and FHV October 2019 data, find the least frequent pickup zone\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pul.Zone,\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhv_2019_10 fhv LEFT JOIN zones pul ON fhv.PUlocationID = pul.LocationID\n",
    "GROUP BY \n",
    "    1\n",
    "ORDER BY\n",
    "    2 ASC\n",
    "LIMIT 5;\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683d3c2-ec2a-4a1b-8b69-add025f08867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
